{"cells":[{"cell_type":"markdown","metadata":{"id":"aFTDBD5g3_zB"},"source":["**V9 **\n","* from V8.1: fixed the save model and resume model code\n","* based on V7 + args similar as V5\n"," - train batch size 24\n"," - transform same as V5\n","* ensure the generated images match the input size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19067,"status":"ok","timestamp":1736613963287,"user":{"displayName":"Kanishk Choudhary","userId":"01620585656264228688"},"user_tz":480},"id":"ve_CVM7MmG5C","outputId":"ba75c023-8c7d-4225-fbfa-49de12e17252"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39089,"status":"ok","timestamp":1736148728071,"user":{"displayName":"Kanishk Choudhary","userId":"01620585656264228688"},"user_tz":480},"id":"WbuSW2x0mJXM","outputId":"90c395ff-ac4b-4750-9b1d-d54dd2180018"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/diffusers\n","  Cloning https://github.com/huggingface/diffusers to /tmp/pip-req-build-smjeb0vn\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers /tmp/pip-req-build-smjeb0vn\n","  Resolved https://github.com/huggingface/diffusers to commit b5726358cf125f2fa1a596dce321e91a225a57e4\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.33.0.dev0) (8.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.33.0.dev0) (3.16.1)\n","Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.33.0.dev0) (0.27.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.33.0.dev0) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.33.0.dev0) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.33.0.dev0) (2.32.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.33.0.dev0) (0.4.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.33.0.dev0) (11.0.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.33.0.dev0) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.33.0.dev0) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.33.0.dev0) (6.0.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.33.0.dev0) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.33.0.dev0) (4.12.2)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata->diffusers==0.33.0.dev0) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.33.0.dev0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.33.0.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.33.0.dev0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.33.0.dev0) (2024.12.14)\n","Building wheels for collected packages: diffusers\n","  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for diffusers: filename=diffusers-0.33.0.dev0-py3-none-any.whl size=3226303 sha256=c0dbab58ee0149b23a38313b489527ab41379ba2232970c084bc70fce6a3c20c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-m7p6ndc4/wheels/f7/7d/99/d361489e5762e3464b3811bc629e94cf5bf5ef44dd5c3c4d52\n","Successfully built diffusers\n","Installing collected packages: diffusers\n","  Attempting uninstall: diffusers\n","    Found existing installation: diffusers 0.31.0\n","    Uninstalling diffusers-0.31.0:\n","      Successfully uninstalled diffusers-0.31.0\n","Successfully installed diffusers-0.33.0.dev0\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n","Collecting wand\n","  Downloading Wand-0.6.13-py2.py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.27.0)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n","Downloading Wand-0.6.13-py2.py3-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wand\n","Successfully installed wand-0.6.13\n","Requirement already satisfied: accelerate>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 1)) (1.2.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 2)) (0.20.1+cu121)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 3)) (4.47.1)\n","Collecting datasets>=2.19.1 (from -r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4))\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Collecting ftfy (from -r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 5))\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 6)) (2.17.1)\n","Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 7)) (3.1.4)\n","Collecting peft==0.7.0 (from -r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8))\n","  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (2.5.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (4.67.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (0.4.5)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (0.27.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 2)) (11.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (3.4.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8)) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 3)) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 3)) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 3)) (0.21.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4))\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (2.2.2)\n","Collecting xxhash (from datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4))\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4))\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec (from torch>=1.13.0->peft==0.7.0->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 8))\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (3.11.10)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 5)) (0.2.13)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 6)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 6)) (1.68.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 6)) (3.7)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 6)) (4.25.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 6)) (75.1.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 6)) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 6)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 6)) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 7)) (3.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (1.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 3)) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 3)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 3)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 3)) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.19.1->-r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt (line 4)) (2024.2)\n","Downloading peft-0.7.0-py3-none-any.whl (168 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, ftfy, fsspec, dill, multiprocess, peft, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","  Attempting uninstall: peft\n","    Found existing installation: peft 0.14.0\n","    Uninstalling peft-0.14.0:\n","      Successfully uninstalled peft-0.14.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 ftfy-6.3.1 multiprocess-0.70.16 peft-0.7.0 xxhash-3.5.0\n","accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"]}],"source":["!pip install git+https://github.com/huggingface/diffusers\n","!pip install accelerate wand\n","!pip install -r https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/requirements.txt\n","\n","!accelerate config default"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0pPLqKMmMej","colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["708245e4b9954d0ca1131a05cd2da201","d25e93227ed446ad85a8ff76ebd50cbf","ccf77963afb54a449bc3c11235af81f5","c77ec2d955924b1cb291612b5b79324a","734545cbc5434f0aad07dc26d198d43e","5e245716fb554e91809562a142b734c1","df4fe1d436fa429f827d873c2aa4d063","a24e2fdcdd684bcd84d9b20017480029","e18b9af3645a4a5184cc9e9e7553c690","0846753e03204546a8ecc0b4c8065df9","6bb0a080b7854944b9421b932c20ae40"]},"executionInfo":{"status":"ok","timestamp":1736613981277,"user_tz":480,"elapsed":17991,"user":{"displayName":"Kanishk Choudhary","userId":"01620585656264228688"}},"outputId":"6a761bb7-ea6d-4a75-a815-72c9bd54ce97"},"outputs":[{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"708245e4b9954d0ca1131a05cd2da201"}},"metadata":{}}],"source":["import wandb\n","import torch\n","from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, AutoPipelineForText2Image\n","from huggingface_hub import model_info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HgKMiOTCZb1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736614073633,"user_tz":480,"elapsed":5140,"user":{"displayName":"Kanishk Choudhary","userId":"01620585656264228688"}},"outputId":"ebe8b007-68ae-45c4-f123-2202076e60fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip install datasets # Install the datasets library\n","\"\"\"Fine-tuning script for Stable Diffusion for text2image with support for LoRA.\"\"\"\n","\n","import argparse\n","import logging\n","import math\n","import os\n","import random\n","import shutil\n","from contextlib import nullcontext\n","from pathlib import Path\n","\n","import datasets\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.utils.checkpoint\n","import transformers\n","from accelerate import Accelerator\n","from accelerate.logging import get_logger\n","from accelerate.utils import ProjectConfiguration, set_seed\n","from datasets import load_dataset\n","from huggingface_hub import create_repo, upload_folder\n","from packaging import version\n","from peft import LoraConfig\n","from peft.utils import get_peft_model_state_dict\n","from torchvision import transforms\n","from tqdm.auto import tqdm\n","from transformers import CLIPTextModel, CLIPTokenizer\n","\n","import diffusers\n","from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n","from diffusers.optimization import get_scheduler\n","from diffusers.training_utils import cast_training_params, compute_snr\n","from diffusers.utils import check_min_version, convert_state_dict_to_diffusers, is_wandb_available\n","from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n","from diffusers.utils.import_utils import is_xformers_available\n","from diffusers.utils.torch_utils import is_compiled_module\n","\n","\n","if is_wandb_available():\n","    import wandb\n","\n","# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\n","check_min_version(\"0.32.0.dev0\")\n","\n","logger = get_logger(__name__, log_level=\"INFO\")\n","\n","\n","def save_model_card(\n","    repo_id: str,\n","    images: list = None,\n","    base_model: str = None,\n","    dataset_name: str = None,\n","    repo_folder: str = None,\n","):\n","    img_str = \"\"\n","    if images is not None:\n","        for i, image in enumerate(images):\n","            image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n","            img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n","\n","    model_description = f\"\"\"\n","# LoRA text2image fine-tuning - {repo_id}\n","These are LoRA adaption weights for {base_model}. The weights were fine-tuned on the {dataset_name} dataset. You can find some example images in the following. \\n\n","{img_str}\n","\"\"\"\n","\n","    model_card = load_or_create_model_card(\n","        repo_id_or_path=repo_id,\n","        from_training=True,\n","        license=\"creativeml-openrail-m\",\n","        base_model=base_model,\n","        model_description=model_description,\n","        inference=True,\n","    )\n","\n","    tags = [\n","        \"stable-diffusion\",\n","        \"stable-diffusion-diffusers\",\n","        \"text-to-image\",\n","        \"diffusers\",\n","        \"diffusers-training\",\n","        \"lora\",\n","    ]\n","    model_card = populate_model_card(model_card, tags=tags)\n","\n","    model_card.save(os.path.join(repo_folder, \"README.md\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PppJmg-GDU9Q"},"outputs":[],"source":["def parse_args():\n","    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n","    parser.add_argument(\n","        \"--pretrained_model_name_or_path\",\n","        type=str,\n","        default=None,\n","        required=True,\n","        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n","    )\n","    parser.add_argument(\n","        \"--revision\",\n","        type=str,\n","        default=None,\n","        required=False,\n","        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n","    )\n","    parser.add_argument(\n","        \"--variant\",\n","        type=str,\n","        default=None,\n","        help=\"Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16\",\n","    )\n","    parser.add_argument(\n","        \"--dataset_name\",\n","        type=str,\n","        default=None,\n","        help=(\n","            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n","            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n","            \" or to a folder containing files that ðŸ¤— Datasets can understand.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--dataset_config_name\",\n","        type=str,\n","        default=None,\n","        help=\"The config of the Dataset, leave as None if there's only one config.\",\n","    )\n","    parser.add_argument(\n","        \"--train_data_dir\",\n","        type=str,\n","        default=None,\n","        help=(\n","            \"A folder containing the training data. Folder contents must follow the structure described in\"\n","            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n","            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--image_column\", type=str, default=\"image\", help=\"The column of the dataset containing an image.\"\n","    )\n","    parser.add_argument(\n","        \"--caption_column\",\n","        type=str,\n","        default=\"text\",\n","        help=\"The column of the dataset containing a caption or a list of captions.\",\n","    )\n","    parser.add_argument(\n","        \"--validation_prompt\", type=str, default=None, help=\"A prompt that is sampled during training for inference.\"\n","    )\n","    parser.add_argument(\n","        \"--num_validation_images\",\n","        type=int,\n","        default=4,\n","        help=\"Number of images that should be generated during validation with `validation_prompt`.\",\n","    )\n","    parser.add_argument(\n","        \"--validation_epochs\",\n","        type=int,\n","        default=1,\n","        help=(\n","            \"Run fine-tuning validation every X epochs. The validation process consists of running the prompt\"\n","            \" `args.validation_prompt` multiple times: `args.num_validation_images`.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--max_train_samples\",\n","        type=int,\n","        default=None,\n","        help=(\n","            \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n","            \"value if set.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--output_dir\",\n","        type=str,\n","        default=\"sd-model-finetuned-lora\",\n","        help=\"The output directory where the model predictions and checkpoints will be written.\",\n","    )\n","    parser.add_argument(\n","        \"--cache_dir\",\n","        type=str,\n","        default=None,\n","        help=\"The directory where the downloaded models and datasets will be stored.\",\n","    )\n","    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n","    parser.add_argument(\n","        \"--resolution\",\n","        type=int,\n","        default=512,\n","        help=(\n","            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n","            \" resolution\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--center_crop\",\n","        default=False,\n","        action=\"store_true\",\n","        help=(\n","            \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n","            \" cropped. The images will be resized to the resolution first before cropping.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--random_flip\",\n","        action=\"store_true\",\n","        help=\"whether to randomly flip images horizontally\",\n","    )\n","    parser.add_argument(\n","        \"--train_batch_size\", type=int, default=16, help=\"Batch size (per device) for the training dataloader.\"\n","    )\n","    parser.add_argument(\"--num_train_epochs\", type=int, default=15) # V5_TEMP was 100\n","    parser.add_argument(\n","        \"--max_train_steps\",\n","        type=int,\n","        default=None,\n","        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n","    )\n","    parser.add_argument(\n","        \"--gradient_accumulation_steps\",\n","        type=int,\n","        default=1,\n","        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n","    )\n","    parser.add_argument(\n","        \"--gradient_checkpointing\",\n","        action=\"store_true\",\n","        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n","    )\n","    parser.add_argument(\n","        \"--learning_rate\",\n","        type=float,\n","        default=1e-4,\n","        help=\"Initial learning rate (after the potential warmup period) to use.\",\n","    )\n","    parser.add_argument(\n","        \"--scale_lr\",\n","        action=\"store_true\",\n","        default=False,\n","        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n","    )\n","    parser.add_argument(\n","        \"--lr_scheduler\",\n","        type=str,\n","        default=\"constant\",\n","        help=(\n","            'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n","            ' \"constant\", \"constant_with_warmup\"]'\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--lr_warmup_steps\", type=int, default=500, help=\"Number of steps for the warmup in the lr scheduler.\"\n","    )\n","    parser.add_argument(\n","        \"--snr_gamma\",\n","        type=float,\n","        default=None,\n","        help=\"SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. \"\n","        \"More details here: https://arxiv.org/abs/2303.09556.\",\n","    )\n","    parser.add_argument(\n","        \"--use_8bit_adam\", action=\"store_true\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n","    )\n","    parser.add_argument(\n","        \"--allow_tf32\",\n","        action=\"store_true\",\n","        help=(\n","            \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n","            \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--dataloader_num_workers\",\n","        type=int,\n","        default=0,\n","        help=(\n","            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n","        ),\n","    )\n","    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n","    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n","    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n","    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n","    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n","    parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n","    parser.add_argument(\n","        \"--prediction_type\",\n","        type=str,\n","        default=None,\n","        help=\"The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or leave `None`. If left to `None` the default prediction type of the scheduler: `noise_scheduler.config.prediction_type` is chosen.\",\n","    )\n","    parser.add_argument(\n","        \"--hub_model_id\",\n","        type=str,\n","        default=None,\n","        help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n","    )\n","    parser.add_argument(\n","        \"--logging_dir\",\n","        type=str,\n","        default=\"logs\",\n","        help=(\n","            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n","            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--mixed_precision\",\n","        type=str,\n","        default=None,\n","        choices=[\"no\", \"fp16\", \"bf16\"],\n","        help=(\n","            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n","            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n","            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--report_to\",\n","        type=str,\n","        default=\"tensorboard\",\n","        help=(\n","            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n","            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n","        ),\n","    )\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n","    parser.add_argument(\n","        \"--checkpointing_steps\",\n","        type=int,\n","        default=500,\n","        help=(\n","            \"Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming\"\n","            \" training using `--resume_from_checkpoint`.\"\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--checkpoints_total_limit\",\n","        type=int,\n","        default=None,\n","        help=(\"Max number of checkpoints to store.\"),\n","    )\n","    parser.add_argument(\n","        \"--resume_from_checkpoint\",\n","        type=str,\n","        default=None,\n","        help=(\n","            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n","            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n","        ),\n","    )\n","    parser.add_argument(\n","        \"--enable_xformers_memory_efficient_attention\", action=\"store_true\", help=\"Whether or not to use xformers.\"\n","    )\n","    parser.add_argument(\"--noise_offset\", type=float, default=0, help=\"The scale of noise offset.\")\n","    parser.add_argument(\n","        \"--rank\",\n","        type=int,\n","        default=4,\n","        help=(\"The dimension of the LoRA update matrices.\"),\n","    )\n","    parser.add_argument(\n","        \"--validation_prompts\",\n","        type=str,\n","        nargs=\"+\",  # Allows multiple prompts as arguments\n","        default=[\"a histopathological image of an area with adipose\",\n","                 \"a histopathological image of an area with mucus\",\n","                 \"a histopathological image of an area with cancer-associated stroma\",\n","                 \"a histopathological image of an area with smooth muscle\",\n","                 \"a histopathological image of an area with colorectal adenocarcinoma epithelium\",\n","                 \"a histopathological image of an area with lymphocytes\",\n","                 \"a histopathological image of an area with debris\",\n","                 \"a histopathological image of an area with background\",\n","                 \"a histopathological image of an area with normal colon mucosa\"], # prompts\n","        help=\"A list of prompts that is sampled during training for inference.\"\n","    )\n","\n","    args = parser.parse_args()\n","    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n","    if env_local_rank != -1 and env_local_rank != args.local_rank:\n","        args.local_rank = env_local_rank\n","\n","    # Sanity checks\n","    if args.dataset_name is None and args.train_data_dir is None:\n","        raise ValueError(\"Need either a dataset name or a training folder.\")\n","\n","    return args\n","\n","\n","DATASET_NAME_MAPPING = {\n","    \"lambdalabs/naruto-blip-captions\": (\"image\", \"text\"),\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NV4ZP17tMlq"},"outputs":[],"source":["import torch\n","import wandb\n","\n","def log_validation(pipeline, args, accelerator, is_final_validation=False, global_step=None):\n","    logger.info(f\"Running validation... \\n Generating {args.num_validation_images} images for each prompt.\")\n","    pipeline = pipeline.to(accelerator.device)  # Move pipeline to device\n","    pipeline.set_progress_bar_config(disable=True)\n","    generator = torch.Generator(device=accelerator.device)\n","    if args.seed is not None:\n","        generator = generator.manual_seed(args.seed)\n","\n","    if torch.backends.mps.is_available():\n","        autocast_ctx = nullcontext()\n","    else:\n","        autocast_ctx = torch.autocast(accelerator.device.type)\n","\n","    # Get weight_dtype based on mixed_precision setting\n","    weight_dtype = torch.float32\n","    if accelerator.mixed_precision == \"fp16\":\n","        weight_dtype = torch.float16\n","    elif accelerator.mixed_precision == \"bf16\":\n","        weight_dtype = torch.bfloat16\n","\n","    # Ensure safety checker is in the correct precision and device\n","    pipeline.safety_checker = pipeline.safety_checker.to(\n","        dtype=weight_dtype, device=accelerator.device\n","    )\n","    pipeline.safety_checker = None\n","    pipeline.requires_safety_checker = False\n","\n","    all_images = []  # To store images for all prompts\n","    for prompt in args.validation_prompts:\n","        logger.info(f\"Generating images for prompt: {prompt}\")\n","        images = []  # To store images for the current prompt\n","        with autocast_ctx:\n","            for _ in range(args.num_validation_images):\n","                # Add height and width arguments here to control the output image size\n","                images.append(pipeline(prompt, num_inference_steps=30, generator=generator, height=args.resolution, width=args.resolution).images[0])\n","\n","        all_images.append((prompt, images))  # Store prompt and images together\n","\n","    log_dict = {}  # Empty dictionary for logging\n","\n","    for tracker in accelerator.trackers:\n","        phase_name = \"test\" if is_final_validation else \"validation\"\n","        for prompt, images in all_images:  # Iterate through prompts and images\n","            # Log images to wandb using accelerator.log\n","            for i, image in enumerate(images):\n","                image_key = f\"{phase_name}/images/{prompt}/{i}\"\n","                wandb.log({image_key: wandb.Image(image)}, step=global_step)  # Use wandb.log here\n","               # image = image.to(accelerator.device)  # Move image to the correct device if needed\n","               # log_dict[image_key] = wandb.Image(image, caption=f\"{i}: {prompt}\") # Using wandb.Image for logging images\n","  #  accelerator.log(log_dict, step=global_step)  # Log using accelerator.log\n","\n","    return all_images"]},{"cell_type":"code","source":["import sys # V7\n","\n","sys.argv = [\n","    '--mixed_precision=bfp16',  # V7 changed from fp16 (gemini recommended)\n","    '--pretrained_model_name_or_path=stable-diffusion-v1-5/stable-diffusion-v1-5',\n","    '--train_data_dir=/content/drive/MyDrive/pathmnist/images',\n","    '--dataloader_num_workers=8',\n","    '--resolution=224',\n","    '--train_batch_size=24',  # from V5\n","    '--gradient_accumulation_steps=4',\n","    '--max_train_steps=3000',\n","    '--learning_rate=5e-5',\n","    #'--gradient_checkpointing', saves memory saving; instead save compute time on A100\n","    '--max_grad_norm=0.5',\n","    '--lr_scheduler=constant_with_warmup', # not used\n","    '--lr_warmup_steps=200', # not used\n","    '--output_dir=/content/drive/MyDrive/pathmnist/generation_model/',\n","    '--checkpointing_steps=500',\n","    '--report_to=wandb',\n","    '--caption_column=label',\n","    '--seed=1337',\n","    '--adam_weight_decay=0.0001',\n","    '--num_validation_images=10',\n","    '--noise_offset=0.15',  # V7 restoring to 0.15 from 0.2; looks like image details degraded?\n","    '--resume_from_checkpoint=latest'\n","]"],"metadata":{"id":"qy2fCWOTU1Yi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94539,"status":"ok","timestamp":1736614190150,"user":{"displayName":"Kanishk Choudhary","userId":"01620585656264228688"},"user_tz":480},"id":"1lnrSDSqnr81","outputId":"717ad878-98d0-409f-b255-c0e8643705db"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 52\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["!wandb login"]},{"cell_type":"code","source":["from diffusers.models.lora import LoRACompatibleLinear\n","\n","def save_model(pipeline, accelerator, global_step):\n","    # Unwrap the pipeline\n","    base_pipeline = accelerator.unwrap_model(pipeline)\n","\n","    # Get the state dict of the unet, extracting only the LoRA parameters\n","    unet_lora_state_dict = get_peft_model_state_dict(base_pipeline.unet, state_dict=base_pipeline.unet.state_dict())\n","\n","    # Save the rest of the pipeline\n","    checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n","    base_pipeline.save_pretrained(checkpoint_dir, safe_serialization=True)\n","\n","    # Now get the unet's save directory, created by save_pretrained\n","    unet_save_dir = os.path.join(checkpoint_dir, \"unet\")\n","\n","    # Save the LoRA weights in the unet's save directory\n","    os.makedirs(unet_save_dir, exist_ok=True)\n","    lora_weights_path = os.path.join(unet_save_dir, \"adapter_model.bin\")  # Usual filename for LoRA weights\n","    safetensors.torch.save_file(unet_lora_state_dict, lora_weights_path)\n","\n","    # Save the accelerator state\n","    accelerator.save_state(checkpoint_dir)\n","\n","def load_model(pipeline, accelerator):\n","    # Load the most recent checkpoint\n","    dirs = os.listdir(args.output_dir)\n","    dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n","    dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n","\n","    # Check if dirs is empty and handle the case\n","    if not dirs:\n","        print(\"No checkpoints found in the output directory. Starting training from scratch.\")\n","        return pipeline, None  # Or raise an exception if you prefer\n","\n","    latest_checkpoint = dirs[-1]\n","    checkpoint_dir = os.path.join(args.output_dir, latest_checkpoint)\n","\n","    # Load the pipeline using DiffusionPipeline.from_pretrained\n","    pipeline = DiffusionPipeline.from_pretrained(\n","       checkpoint_dir,\n","       unet=unwrap_model(unet),\n","       torch_dtype=weight_dtype,\n","    ).to(\"cuda\")\n","\n","    # Load weights from .safetensors file (for U-Net)\n","    #state_dict = safetensors.torch.load_file(os.path.join(os.path.join(args.output_dir, path), \"pytorch_lora_weights.safetensors\"))\n","    #unwrap_model(unet).load_state_dict(state_dict, strict=False)  # Apply to the unwrapped model\n","    # Load LoRA weights using safetensors.torch.load_file\n","    lora_weights_path = os.path.join(args.output_dir, latest_checkpoint, \"unet\", \"adapter_model.bin\")\n","    lora_state_dict = safetensors.torch.load_file(lora_weights_path)\n","\n","    # Apply LoRA state dict to the U-Net\n","    for name, module in pipeline.unet.named_modules():\n","        if isinstance(module, LoRACompatibleLinear):\n","            lora_key = f\"{name}.lora_A.weight\"  # Adjust if LoRA keys are different\n","            if lora_key in lora_state_dict:\n","                module.lora_A.weight.data = lora_state_dict[lora_key]\n","\n","    # Load the accelerator state\n","    accelerator.load_state(checkpoint_dir)\n","\n","    return pipeline, latest_checkpoint"],"metadata":{"id":"moFKW37FAOZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["52fcc58c6e5340698ede98fedb2b99e4","30f922f4f56d43daa5c1d8e6c3e30aa2","209ed205a77945ac843a4eb7ab2f67f4","d21f20a6fb654f9c8991441b869ae061","a9d29a298bc94020b347b186b3f037ba","42191a6eac514141a55d45cabb0bf070","54bb120a2d944e72b1f82cc0a1f220fc","31240efdaefc47319c684dee98dcb1a6","01c2171e9aaf417f8832c98df6cafbbe","9043727c210e404f88704771ff913ba8","50e5b2abacb34c528d5de9dd67e9b631","662f5460f42a4f0baa44564dc23e15a1","214d74153de54064a266242571013289","c0b314c451d240899d78b820fed77c80","b4563ed1c9fb480480062fb3cd635b54","c96d2e5eda12476e981986e59808817e","6bd64f8a1eec477d9d40349771938641","d73d581218ff4a7ca5084cde11769b88","54ec6a7b15964f3da687040f956344b5","1ea00da510e4432ea69f14a318599f14","ceec8859b20b4e6a832b1dbb2f7d5488","1734bf7f48384ebba9399fa3daf91d29","e9166eda25da48b38fc9150f608fcbd2","0ca3cce261c94a02ac580aef759abbc8","8f233051ddb742a190fe2f3d78a9701d","67814d7a850b42c18dbc983065b8e35d","7db766b6bbd34887a0de3cbdadc20ec1","b09ff9fd05274a8e9667bdb0e79c0205","3b7cba5695ba4dbdb406518e76738555","b84d8b92ab65465ba85872adacfa5ad2","1c01b9c5b7c64599ad460b514d4d7e97","f2fb57ba379b4e45aafe44b462188eaa","2544bbb330934cfebf7e18d67084f543","be2285b8cc024fd7a04d6908aad9731f","819a5ecf3c374ef79dd3929c78d31a91","20e2140f05744197b054408221812f3e","4034ae7513e04f79b77ec82fd7b6034f","b7fd644f4b554d2a99b207d45cb6e519","94ff5d1109e44e96b2eb5216b8b722ef","e5b1d6af502c40f0aa054f11d36a65fb","b183c3ab2e864e69aebda4554b810649","bc3363a1851b43d8a0c878d6c0de2722","f55be18b63a840538f3d08dee0d66d96","bc3a72e83bb94fe6bed3e8576a20263d","16f91b37b889457d83a856c01258f1e3","a6cf9ed2458743cfba61d3ec6cbf6041","4c233904d6814842ada362ad0b54f971","a1850871c3b84f709bc9cc5d14b7aadd","35faa8623e9643cc8933daeba5d36103","101a33f7020f4e29904a227370aecd23","b8bee9e790064ca09f53cf2bcbe74e0d","e7c38ee687ba4e1db14c911bbada3fb7","3dc59323c43d474592df291ec6e8c720","dbd9215c92c043fe8adf3dd565e466da","37b39a0c03e34154947caf3a4c0baedb","09922d3957234d258d4bd708c8da4894","748136f70cde4f86992410ec6cf565a0","7f12e253d261464e8b9f16b55608952e","2539e00fe39f48789ed7c80ed66e34a8","3a51a62565fd4c489cef9e77c5dd11e0","8912cd9f384f462cb6c3cdbc7b135ea9","3ae3bb5b1aff40fbb406f5103cb2cb58","73c77a4500004ee69fd806568f595da1","5d3dee912e7e471d900f23a39f4c114e","32ee7b8752a845f58c93294574370791","d02bdb18f31e469da22949bba69854cb","4448b6c19d0942da9ea6d165faf7fa48","957d694b9f1646a59a54cff777842dea","dbc3c96dcf604ee1b8654ddad58909d4","8035d6de11f646169728374b7ccba814","c0e1ed8f9316457092d6539568d024b9","f3d9bc0a03194ed3b371881598f37ad1","f2fdc3475f40441f91a4f94728f53ab0","c74fc042019147d8898149699838adf1","a7488ca13df04f568668ff04f35a55f2","c87f32bb54d44676b69b2cd2cd744336","feb84beefe13452580a2a871890643e7","79d49e6c9dc6452fa8d7e4b6d496207b","84c3c87faf544c44ac45d4fe13827067","f03018064aab47ab8b81cd66eb2ca0e9","294494be69b04753ab1e053bf0836509","82d80e8766b44752addf05dfb1543eb2","332d0dccc6c343829aba89af985897d8","e452e9cea9a740ac84f4d0a7c20dee39","df54c1abb9ee4b1ab378b1a65451aee7","52f71ef716854f4f9e23df1dbd24c6cb","d15761701be24f28aadd90ca6519e337","537bda4539164366a65d0d26ae5c683e"]},"id":"ticJpZjfDevp","outputId":"af72e2db-b4e4-4de5-d23b-51aace9d6a0f","executionInfo":{"status":"error","timestamp":1736623211195,"user_tz":480,"elapsed":2949235,"user":{"displayName":"Kanishk Choudhary","userId":"01620585656264228688"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:Distributed environment: NO\n","Num processes: 1\n","Process index: 0\n","Local process index: 0\n","Device: cuda\n","\n","Mixed precision type: no\n","\n","{'variance_type', 'clip_sample_range', 'sample_max_value', 'prediction_type', 'timestep_spacing', 'rescale_betas_zero_snr', 'thresholding', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n"]},{"output_type":"stream","name":"stdout","text":["args.mixed_precision=None\n"]},{"output_type":"stream","name":"stderr","text":["{'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'latents_mean', 'force_upcast', 'latents_std', 'scaling_factor', 'shift_factor'} was not found in config. Values will be initialized to default values.\n","{'mid_block_type', 'conv_in_kernel', 'dual_cross_attention', 'time_embedding_type', 'time_cond_proj_dim', 'time_embedding_dim', 'class_embeddings_concat', 'transformer_layers_per_block', 'addition_embed_type', 'time_embedding_act_fn', 'timestep_post_act', 'reverse_transformer_layers_per_block', 'addition_time_embed_dim', 'resnet_time_scale_shift', 'conv_out_kernel', 'only_cross_attention', 'resnet_out_scale_factor', 'attention_type', 'addition_embed_type_num_heads', 'num_class_embeds', 'cross_attention_norm', 'upcast_attention', 'encoder_hid_dim', 'encoder_hid_dim_type', 'mid_block_only_cross_attention', 'projection_class_embeddings_input_dim', 'class_embed_type', 'num_attention_heads', 'resnet_skip_time_act', 'use_linear_projection', 'dropout'} was not found in config. Values will be initialized to default values.\n"]},{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/60841 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52fcc58c6e5340698ede98fedb2b99e4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:__main__:***** Running training *****\n","INFO:__main__:  Num examples = 60840\n","INFO:__main__:  Num Epochs = 5\n","INFO:__main__:  Instantaneous batch size per device = 24\n","INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 96\n","INFO:__main__:  Gradient Accumulation steps = 4\n","INFO:__main__:  Total optimization steps = 3000\n","{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"662f5460f42a4f0baa44564dc23e15a1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'latents_mean', 'force_upcast', 'latents_std', 'scaling_factor', 'shift_factor'} was not found in config. Values will be initialized to default values.\n","Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n","Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9166eda25da48b38fc9150f608fcbd2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of /content/drive/MyDrive/pathmnist/generation_model/checkpoint-1500.\n","Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of /content/drive/MyDrive/pathmnist/generation_model/checkpoint-1500.\n","Loaded vae as AutoencoderKL from `vae` subfolder of /content/drive/MyDrive/pathmnist/generation_model/checkpoint-1500.\n","Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of /content/drive/MyDrive/pathmnist/generation_model/checkpoint-1500.\n","Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of /content/drive/MyDrive/pathmnist/generation_model/checkpoint-1500.\n","Loaded scheduler as PNDMScheduler from `scheduler` subfolder of /content/drive/MyDrive/pathmnist/generation_model/checkpoint-1500.\n","INFO:accelerate.accelerator:Loading states from /content/drive/MyDrive/pathmnist/generation_model/checkpoint-1500\n","INFO:accelerate.checkpointing:All model weights loaded successfully\n","INFO:accelerate.checkpointing:All optimizer states loaded successfully\n","INFO:accelerate.checkpointing:All scheduler states loaded successfully\n","INFO:accelerate.checkpointing:All dataloader sampler states loaded successfully\n","INFO:accelerate.checkpointing:All random states loaded successfully\n","INFO:accelerate.accelerator:Loading in 0 custom states\n"]},{"output_type":"stream","name":"stdout","text":["Resumed from checkpoint: checkpoint-1500\n"]},{"output_type":"display_data","data":{"text/plain":["Steps:  50%|#####     | 1500/3000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be2285b8cc024fd7a04d6908aad9731f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1501 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1502 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1503 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1504 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1505 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1506 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1507 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1508 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1509 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1510 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1511 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1512 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1513 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1514 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1515 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1516 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1517 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1518 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1519 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1520 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1521 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1522 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1523 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1524 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1525 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1526 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1527 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1528 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1529 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1530 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1531 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1532 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1533 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1534 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1535 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1536 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1537 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1538 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1539 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1540 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1541 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1542 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1543 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1544 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1545 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1546 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1547 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1548 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1549 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1550 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1551 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1552 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1553 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1554 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1555 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1556 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1557 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1558 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1559 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1560 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1561 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1562 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1563 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1564 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1565 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1566 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1567 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1568 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1569 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1570 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1571 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1572 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1573 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1574 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1575 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1576 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1577 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1578 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1579 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1580 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1581 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1582 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1583 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1584 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1585 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1586 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1587 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1588 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1589 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1590 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1591 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1592 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1593 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1594 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1595 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1596 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1597 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1598 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1599 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1600 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1601 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1602 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1603 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1604 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1605 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1606 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1607 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1608 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1609 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1610 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1611 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1612 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1613 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1614 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1615 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1616 that is less than the current step 1617. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f91b37b889457d83a856c01258f1e3"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'latents_mean', 'force_upcast', 'latents_std', 'scaling_factor', 'shift_factor'} was not found in config. Values will be initialized to default values.\n","Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n","Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Configuration saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/vae/config.json\n","Model weights saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/vae/diffusion_pytorch_model.safetensors\n","Configuration saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/unet/config.json\n","Model weights saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/unet/diffusion_pytorch_model.safetensors\n","Configuration saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/scheduler/scheduler_config.json\n","Configuration saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/model_index.json\n","INFO:accelerate.accelerator:Saving current state to /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000\n","INFO:accelerate.checkpointing:Model weights saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/model.safetensors\n","INFO:accelerate.checkpointing:Optimizer state saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/optimizer.bin\n","INFO:accelerate.checkpointing:Scheduler state saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/scheduler.bin\n","INFO:accelerate.checkpointing:Sampler state for dataloader 0 saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/sampler.bin\n","INFO:accelerate.checkpointing:Random states saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2000/random_states_0.pkl\n","INFO:__main__:Checkpoint saved at step: 2000\n"]},{"output_type":"stream","name":"stdout","text":["Checkpoint saved at step: 2000\n"]},{"output_type":"stream","name":"stderr","text":["{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09922d3957234d258d4bd708c8da4894"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'latents_mean', 'force_upcast', 'latents_std', 'scaling_factor', 'shift_factor'} was not found in config. Values will be initialized to default values.\n","Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n","Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","INFO:__main__:Running validation... \n"," Generating 10 images for each prompt.\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with adipose\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with mucus\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with cancer-associated stroma\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with smooth muscle\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with colorectal adenocarcinoma epithelium\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with lymphocytes\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with debris\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with background\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with normal colon mucosa\n","{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4448b6c19d0942da9ea6d165faf7fa48"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'latents_mean', 'force_upcast', 'latents_std', 'scaling_factor', 'shift_factor'} was not found in config. Values will be initialized to default values.\n","Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n","Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Configuration saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/vae/config.json\n","Model weights saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/vae/diffusion_pytorch_model.safetensors\n","Configuration saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/unet/config.json\n","Model weights saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/unet/diffusion_pytorch_model.safetensors\n","Configuration saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/scheduler/scheduler_config.json\n","Configuration saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/model_index.json\n","INFO:accelerate.accelerator:Saving current state to /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500\n","INFO:accelerate.checkpointing:Model weights saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/model.safetensors\n","INFO:accelerate.checkpointing:Optimizer state saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/optimizer.bin\n","INFO:accelerate.checkpointing:Scheduler state saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/scheduler.bin\n","INFO:accelerate.checkpointing:Sampler state for dataloader 0 saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/sampler.bin\n","INFO:accelerate.checkpointing:Random states saved in /content/drive/MyDrive/pathmnist/generation_model/checkpoint-2500/random_states_0.pkl\n","INFO:__main__:Checkpoint saved at step: 2500\n"]},{"output_type":"stream","name":"stdout","text":["Checkpoint saved at step: 2500\n"]},{"output_type":"stream","name":"stderr","text":["{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79d49e6c9dc6452fa8d7e4b6d496207b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'latents_mean', 'force_upcast', 'latents_std', 'scaling_factor', 'shift_factor'} was not found in config. Values will be initialized to default values.\n","Loaded vae as AutoencoderKL from `vae` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n","Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n","INFO:__main__:Running validation... \n"," Generating 10 images for each prompt.\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with adipose\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with mucus\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with cancer-associated stroma\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with smooth muscle\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with colorectal adenocarcinoma epithelium\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with lymphocytes\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with debris\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with background\n","INFO:__main__:Generating images for prompt: a histopathological image of an area with normal colon mucosa\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-b75efef41f8e>\u001b[0m in \u001b[0;36m<cell line: 423>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;31m# Predict the noise residual and compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0mmodel_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_latents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnr_gamma\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_condition.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid_block\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"has_cross_attention\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m                 sample = self.mid_block(\n\u001b[0m\u001b[1;32m   1247\u001b[0m                     \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m                     \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_emb_proj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["    from diffusers import AutoPipelineForText2Image\n","    import torch\n","    import gc\n","    # from torch.optim.lr_scheduler import LambdaLR\n","    from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","    import safetensors.torch\n","    import random\n","\n","    args = parse_args()\n","    if args.report_to == \"wandb\" and args.hub_token is not None:\n","        raise ValueError(\n","            \"You cannot use both --report_to=wandb and --hub_token due to a security risk of exposing your token.\"\n","            \" Please use `huggingface-cli login` to authenticate with the Hub.\"\n","        )\n","\n","    logging_dir = Path(args.output_dir, args.logging_dir)\n","\n","    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n","\n","    accelerator = Accelerator(\n","        gradient_accumulation_steps=args.gradient_accumulation_steps,\n","        mixed_precision=args.mixed_precision,\n","        log_with=args.report_to,\n","        project_config=accelerator_project_config,\n","    )\n","    print(f\"{args.mixed_precision=}\")\n","\n","    # Disable AMP for MPS.\n","    if torch.backends.mps.is_available():\n","        accelerator.native_amp = False\n","\n","    # Make one log on every process with the configuration for debugging.\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n","    )\n","    logger.info(accelerator.state, main_process_only=False)\n","    if accelerator.is_local_main_process:\n","        datasets.utils.logging.set_verbosity_warning()\n","        transformers.utils.logging.set_verbosity_warning()\n","        diffusers.utils.logging.set_verbosity_info()\n","    else:\n","        datasets.utils.logging.set_verbosity_error()\n","        transformers.utils.logging.set_verbosity_error()\n","        diffusers.utils.logging.set_verbosity_error()\n","\n","    # If passed along, set the training seed now.\n","    if args.seed is not None:\n","        set_seed(args.seed)\n","\n","    # Handle the repository creation\n","    if accelerator.is_main_process:\n","        if args.output_dir is not None:\n","            os.makedirs(args.output_dir, exist_ok=True)\n","\n","        if args.push_to_hub:\n","            repo_id = create_repo(\n","                repo_id=args.hub_model_id or Path(args.output_dir).name, exist_ok=True, token=args.hub_token\n","            ).repo_id\n","    # Load scheduler, tokenizer and models.\n","    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n","    tokenizer = CLIPTokenizer.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n","    )\n","    text_encoder = CLIPTextModel.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n","    )\n","    vae = AutoencoderKL.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision, variant=args.variant\n","    )\n","    unet = UNet2DConditionModel.from_pretrained(\n","        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision, variant=args.variant\n","    )\n","    # freeze parameters of models to save more memory\n","    unet.requires_grad_(False)\n","    vae.requires_grad_(False)\n","    text_encoder.requires_grad_(False)\n","\n","    # For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision\n","    # as these weights are only used for inference, keeping weights in full precision is not required.\n","    weight_dtype = torch.float32\n","    if accelerator.mixed_precision == \"fp16\":\n","        weight_dtype = torch.float16\n","    elif accelerator.mixed_precision == \"bf16\":\n","        weight_dtype = torch.bfloat16\n","\n","    # Freeze the unet parameters before adding adapters\n","    for param in unet.parameters():\n","        param.requires_grad_(False)\n","\n","    unet_lora_config = LoraConfig(\n","        r=args.rank,\n","        lora_alpha=args.rank,\n","        init_lora_weights=\"gaussian\",\n","        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n","    )\n","\n","    # Move unet, vae and text_encoder to device and cast to weight_dtype\n","    unet.to(accelerator.device, dtype=weight_dtype)\n","    vae.to(accelerator.device, dtype=weight_dtype)\n","    text_encoder.to(accelerator.device, dtype=weight_dtype)\n","\n","    # Add adapter and make sure the trainable params are in float32.\n","    unet.add_adapter(unet_lora_config)\n","    if args.mixed_precision == \"fp16\":\n","        # only upcast trainable parameters (LoRA) into fp32\n","        cast_training_params(unet, dtype=torch.float32)\n","\n","    if args.enable_xformers_memory_efficient_attention:\n","        if is_xformers_available():\n","            import xformers\n","\n","            xformers_version = version.parse(xformers.__version__)\n","            if xformers_version == version.parse(\"0.0.16\"):\n","                logger.warning(\n","                    \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n","                )\n","            unet.enable_xformers_memory_efficient_attention()\n","        else:\n","            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n","\n","    lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n","\n","    if args.gradient_checkpointing:\n","        unet.enable_gradient_checkpointing()\n","\n","    # Enable TF32 for faster training on Ampere GPUs,\n","    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n","    if args.allow_tf32:\n","        torch.backends.cuda.matmul.allow_tf32 = True\n","\n","    if args.scale_lr:\n","        args.learning_rate = (\n","            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n","        )\n","\n","    # Initialize the optimizer\n","    if args.use_8bit_adam:\n","        try:\n","            import bitsandbytes as bnb\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n","            )\n","\n","        optimizer_cls = bnb.optim.AdamW8bit\n","    else:\n","        optimizer_cls = torch.optim.AdamW\n","\n","    optimizer = optimizer_cls(\n","        lora_layers,\n","        lr=args.learning_rate,\n","        betas=(args.adam_beta1, args.adam_beta2),\n","        weight_decay=args.adam_weight_decay,\n","        eps=args.adam_epsilon,\n","    )\n","\n","    # Get the datasets: you can either provide your own training and evaluation files (see below)\n","    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n","\n","    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n","    # download the dataset.\n","    if args.dataset_name is not None:\n","        # Downloading and loading a dataset from the hub.\n","        dataset = load_dataset(\n","            args.dataset_name,\n","            args.dataset_config_name,\n","            cache_dir=args.cache_dir,\n","            data_dir=args.train_data_dir,\n","        )\n","    else:\n","        data_files = {}\n","        if args.train_data_dir is not None:\n","            data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n","        dataset = load_dataset(\n","            \"imagefolder\",\n","            data_files=data_files,\n","            cache_dir=args.cache_dir,\n","        )\n","        # See more about loading custom images at\n","        # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\n","\n","    # Preprocessing the datasets.\n","    # We need to tokenize inputs and targets.\n","    column_names = dataset[\"train\"].column_names\n","\n","    # 6. Get the column names for input/target.\n","    dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)\n","    if args.image_column is None:\n","        image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n","    else:\n","        image_column = args.image_column\n","        if image_column not in column_names:\n","            raise ValueError(\n","                f\"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\"\n","            )\n","    if args.caption_column is None:\n","        caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n","    else:\n","        caption_column = args.caption_column\n","        if caption_column not in column_names:\n","            raise ValueError(\n","                f\"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n","            )\n","\n","    # Preprocessing the datasets.\n","    # We need to tokenize input captions and transform the images.\n","    def tokenize_captions(examples, is_train=True):\n","        captions = []\n","        for caption in examples[caption_column]:\n","            if isinstance(caption, str):\n","                captions.append(caption)\n","            elif isinstance(caption, (list, np.ndarray)):\n","                # take a random caption if there are multiple\n","                captions.append(random.choice(caption) if is_train else caption[0])\n","            else:\n","                raise ValueError(\n","                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n","                )\n","        inputs = tokenizer(\n","            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n","        )\n","        return inputs.input_ids\n","\n","    # same as V5\n","    train_transforms = transforms.Compose(\n","        [\n","            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n","            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n","            transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n","            #transforms.RandomRotation(degrees=10),  # Add rotation\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5], [0.5]),\n","        ]\n","    )\n","\n","    def unwrap_model(model):\n","        model = accelerator.unwrap_model(model)\n","        model = model._orig_mod if is_compiled_module(model) else model\n","        return model\n","\n","    def preprocess_train(examples):\n","        images = [image.convert(\"RGB\") for image in examples[image_column]]\n","        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n","        examples[\"input_ids\"] = tokenize_captions(examples)\n","        return examples\n","\n","    with accelerator.main_process_first():\n","        if args.max_train_samples is not None:\n","            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n","        # Set the training transforms\n","        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n","\n","    def collate_fn(examples):\n","        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n","        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n","        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n","        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n","\n","    # DataLoaders creation:\n","    # Assuming 'train_dataset' is your loaded dataset\n","    dataset_size = len(train_dataset)\n","    validation_split = 0.005  # Proportion of data for validation\n","    split_index = int(dataset_size * (1 - validation_split))\n","    indices = list(range(dataset_size))\n","    random.shuffle(indices)  # Shuffle data for randomness\n","\n","    train_indices = indices[:split_index]\n","    validation_indices = indices[split_index:]\n","\n","    train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n","    validation_subset = torch.utils.data.Subset(train_dataset, validation_indices)\n","\n","    # Create data loaders for the subsets\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_subset,\n","        shuffle=True,\n","        collate_fn=collate_fn,  # Assuming you have a collate_fn\n","        batch_size=args.train_batch_size,\n","        num_workers=args.dataloader_num_workers,\n","    )\n","\n","    validation_dataloader = torch.utils.data.DataLoader(\n","        validation_subset,\n","        shuffle=False,  # No need to shuffle validation data\n","        collate_fn=collate_fn,\n","        batch_size=args.train_batch_size,  # You can adjust this\n","        num_workers=args.dataloader_num_workers,\n","    )\n","\n","    # Scheduler and math around the number of training steps.\n","    # Check the PR https://github.com/huggingface/diffusers/pull/8312 for detailed explanation.\n","    num_warmup_steps_for_scheduler = args.lr_warmup_steps * accelerator.num_processes\n","    if args.max_train_steps is None:\n","        len_train_dataloader_after_sharding = math.ceil(len(train_dataloader) / accelerator.num_processes)\n","        num_update_steps_per_epoch = math.ceil(len_train_dataloader_after_sharding / args.gradient_accumulation_steps)\n","        num_training_steps_for_scheduler = (\n","            args.num_train_epochs * num_update_steps_per_epoch * accelerator.num_processes\n","        )\n","    else:\n","        num_training_steps_for_scheduler = args.max_train_steps * accelerator.num_processes\n","\n","    # Reasoning: We replace the default scheduler with a custom LambdaLR scheduler to control the learning rate more precisely.\n","'''\n","    # The lr_lambda function calculates the learning rate based on the current step.\n","    # During warmup, it linearly increases the learning rate to the initial value.\n","    # After warmup, it sets the learning rate to the reduced value.\n","    def lr_lambda(current_step):\n","        if current_step < num_warmup_steps_for_scheduler:\n","            return float(current_step) / float(max(1, num_warmup_steps_for_scheduler))\n","        else:\n","            # Reduced learning rate after warmup\n","            return 1e-5  # Or any other lower value you want to try\n","'''\n","    # V6 => Suggested Custom Scheduler\n","    def lr_lambda(current_step):\n","      if current_step < args.lr_warmup_steps:\n","        return float(current_step) / float(max(1, args.lr_warmup_steps))\n","      progress = float(current_step - args.lr_warmup_steps) / float(max(1, args.max_train_steps - args.lr_warmup_steps))\n","      return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n","\n","    # lr_scheduler = LambdaLR(optimizer, lr_lambda)\n","\n","   # V7 CosineAnnealingWarmRestarts scheduler from PyTorch's torch.optim.lr_scheduler module\n","   # T_0: Number of iterations for the first restart.\n","   # T_mult: A factor increases T_i after a restart. Default: 1.\n","    lr_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=500, T_mult=2)\n","\n","   # lr_scheduler = get_scheduler(\n","   #     args.lr_scheduler,\n","   #     optimizer=optimizer,\n","   #     num_warmup_steps=num_warmup_steps_for_scheduler,\n","   #     num_training_steps=num_training_steps_for_scheduler,\n","   # )\n","\n","    # Prepare everything with our `accelerator`.\n","    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n","        unet, optimizer, train_dataloader, lr_scheduler\n","    )\n","\n","    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n","    if args.max_train_steps is None:\n","        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n","        if num_training_steps_for_scheduler != args.max_train_steps * accelerator.num_processes:\n","            logger.warning(\n","                f\"The length of the 'train_dataloader' after 'accelerator.prepare' ({len(train_dataloader)}) does not match \"\n","                f\"the expected length ({len_train_dataloader_after_sharding}) when the learning rate scheduler was created. \"\n","                f\"This inconsistency may result in the learning rate scheduler not functioning properly.\"\n","            )\n","    # Afterwards we recalculate our number of training epochs\n","    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n","\n","    # We need to initialize the trackers we use, and also store our configuration.\n","    # The trackers initializes automatically on the main process.\n","    if accelerator.is_main_process:\n","        accelerator.init_trackers(\"text2image-fine-tune\", config=vars(args))\n","\n","    # Train!\n","    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n","\n","    logger.info(\"***** Running training *****\")\n","    logger.info(f\"  Num examples = {len(train_dataset)}\")\n","    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n","    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n","    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n","    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n","    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n","    global_step = 0\n","    first_epoch = 0\n","\n","    # Initialize the pipeline before loading the model\n","    pipeline = DiffusionPipeline.from_pretrained(\n","        args.pretrained_model_name_or_path,\n","        unet=unwrap_model(unet),\n","        revision=args.revision,\n","        variant=args.variant,\n","        torch_dtype=weight_dtype,\n","    )\n","    pipeline = pipeline.to(accelerator.device)\n","\n","    # Load model if resuming from checkpoint\n","    if args.resume_from_checkpoint:\n","        pipeline, latest_checkpoint = load_model(pipeline, accelerator)\n","        #load_model(accelerator)\n","        if latest_checkpoint is not None:  # Check if a checkpoint was found\n","            print(f\"Resumed from checkpoint: {latest_checkpoint}\")\n","            initial_global_step = int(latest_checkpoint.split(\"-\")[1]) # Changed path to latest_checkpoint\n","            first_epoch = initial_global_step // num_update_steps_per_epoch\n","\n","            # Access and update wandb run's step counter (if initialized)\n","            import wandb\n","            if accelerator.is_main_process and wandb.run is not None:  # Check for wandb.run\n","              #wandb.log({}, step=initial_global_step)  # Log an empty dictionary to update the step\n","              wandb.run.summary[\"global_step\"] = global_step  # Update wandb's step counter\n","        else:\n","            print(\"No checkpoints found. Starting training from scratch.\")\n","            initial_global_step = 0 # Initialize if no checkpoint\n","\n","    progress_bar = tqdm(\n","        range(0, args.max_train_steps),\n","        initial=initial_global_step,\n","        desc=\"Steps\",\n","        # Only show the progress bar once on each machine.\n","        disable=not accelerator.is_local_main_process,\n","    )\n","\n","    ''' V7 skip as already generated once with V6\n","    # Create pipeline at the start of each epoch if validation prompts are provided\n","    if args.validation_prompts is not None:\n","        pipeline = DiffusionPipeline.from_pretrained(\n","            args.pretrained_model_name_or_path,\n","            unet=unwrap_model(unet),\n","            revision=args.revision,\n","            variant=args.variant,\n","            torch_dtype=weight_dtype,\n","        )\n","    pipeline = pipeline.to(accelerator.device)\n","    log_validation(pipeline, args, accelerator, global_step=0)\n","    '''\n","\n","    for epoch in range(first_epoch, args.num_train_epochs):\n","        unet.train()\n","        train_loss = 0.0\n","\n","        for step, batch in enumerate(train_dataloader):\n","            with accelerator.accumulate(unet):\n","                # Convert images to latent space\n","                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n","                latents = latents * vae.config.scaling_factor\n","\n","                # Sample noise that we'll add to the latents\n","                noise = torch.randn_like(latents)\n","                if args.noise_offset:\n","                    # https://www.crosslabs.org//blog/diffusion-with-offset-noise\n","                    noise += args.noise_offset * torch.randn(\n","                        (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\n","                    )\n","\n","                bsz = latents.shape[0]\n","                # Sample a random timestep for each image\n","                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n","                timesteps = timesteps.long()\n","\n","                # Add noise to the latents according to the noise magnitude at each timestep\n","                # (this is the forward diffusion process)\n","                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","                # Get the text embedding for conditioning\n","                encoder_hidden_states = text_encoder(batch[\"input_ids\"], return_dict=False)[0]\n","\n","                # Get the target for loss depending on the prediction type\n","                if args.prediction_type is not None:\n","                    # set prediction_type of scheduler if defined\n","                    noise_scheduler.register_to_config(prediction_type=args.prediction_type)\n","\n","                if noise_scheduler.config.prediction_type == \"epsilon\":\n","                    target = noise\n","                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n","                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n","                else:\n","                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n","\n","                # Predict the noise residual and compute loss\n","                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n","\n","                if args.snr_gamma is None:\n","                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n","                else:\n","                    # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n","                    # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n","                    # This is discussed in Section 4.2 of the same paper.\n","                    snr = compute_snr(noise_scheduler, timesteps)\n","                    mse_loss_weights = torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(\n","                        dim=1\n","                    )[0]\n","                    if noise_scheduler.config.prediction_type == \"epsilon\":\n","                        mse_loss_weights = mse_loss_weights / snr\n","                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\n","                        mse_loss_weights = mse_loss_weights / (snr + 1)\n","\n","                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n","                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n","                    loss = loss.mean()\n","\n","                # Gather the losses across all processes for logging (if we use distributed training).\n","                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n","                train_loss += avg_loss.item() / args.gradient_accumulation_steps\n","\n","                # Backpropagate\n","                accelerator.backward(loss)\n","                if accelerator.sync_gradients:\n","                    params_to_clip = lora_layers\n","                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n","                optimizer.step()\n","                lr_scheduler.step()\n","                optimizer.zero_grad()\n","\n","            # Checks if the accelerator has performed an optimization step behind the scenes\n","            if accelerator.sync_gradients:\n","                progress_bar.update(1)\n","                global_step += 1\n","                logging_step = global_step + initial_global_step if initial_global_step else global_step\n","                accelerator.log({\"train_loss\": train_loss}, step=logging_step)\n","                train_loss = 0.0\n","\n","                # Save model at checkpointing steps\n","                if accelerator.is_main_process and (global_step % args.checkpointing_steps == 0):\n","                    # Create a new pipeline here\n","                    pipeline_for_saving = DiffusionPipeline.from_pretrained(\n","                        args.pretrained_model_name_or_path,\n","                        unet=unwrap_model(unet),\n","                        torch_dtype=weight_dtype,\n","                    ).to(accelerator.device)\n","\n","                    save_model(pipeline_for_saving, accelerator, logging_step)\n","                    print(f\"Checkpoint saved at step: {logging_step}\") # Add a message to indicate save\n","                    logger.info(f\"Checkpoint saved at step: {logging_step}\")\n","                    # Delete the pipeline\n","                    del pipeline_for_saving\n","                    gc.collect()  # Optional: Force garbage collection\n","\n","            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n","            progress_bar.set_postfix(**logs)\n","\n","            if global_step >= args.max_train_steps:\n","                break\n","\n","        if accelerator.is_main_process:\n","            if args.validation_prompts is not None and epoch % args.validation_epochs == 0:\n","                # create pipeline\n","                pipeline = DiffusionPipeline.from_pretrained(\n","                    args.pretrained_model_name_or_path,\n","                    unet=unwrap_model(unet),\n","                    revision=args.revision,\n","                    variant=args.variant,\n","                    torch_dtype=weight_dtype,\n","                )\n","\n","                unet.eval()  # Set the model to evaluation mode\n","                pipeline = pipeline.to(accelerator.device)\n","                # validation_loss = validate(pipeline, validation_dataloader, accelerator, global_step)\n","                # logger.info(f\"Epoch {epoch+1}/{args.num_train_epochs}, Validation Loss: {validation_loss:.4f}\")\n","                wandb_logging_step = global_step + initial_global_step if initial_global_step else global_step\n","                images = log_validation(pipeline, args, accelerator, is_final_validation=False,  global_step=wandb_logging_step)\n","                unet.train() # Set back to training mode\n","\n","                del pipeline\n","                del loss, noisy_latents, encoder_hidden_states, model_pred  # Delete unnecessary variables\n","                gc.collect()\n","                torch.cuda.empty_cache()\n","\n","    # Save the lora layers\n","    accelerator.wait_for_everyone()\n","    if accelerator.is_main_process:\n","        unet = unet.to(torch.float32)\n","\n","        unwrapped_unet = unwrap_model(unet)\n","        unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))\n","        StableDiffusionPipeline.save_lora_weights(\n","            save_directory=args.output_dir,\n","            unet_lora_layers=unet_lora_state_dict,\n","            safe_serialization=True,\n","        )\n","\n","        # Final inference\n","        # Load previous pipeline\n","        if args.validation_prompts is not None:\n","            pipeline = DiffusionPipeline.from_pretrained(\n","                args.pretrained_model_name_or_path,\n","                revision=args.revision,\n","                variant=args.variant,\n","                torch_dtype=weight_dtype,\n","            )\n","\n","            # load attention processors\n","            pipeline.load_lora_weights(args.output_dir)\n","\n","            # run inference\n","            wandb_logging_step = global_step + initial_global_step if initial_global_step else global_step\n","            images = log_validation(pipeline, args, accelerator, is_final_validation=True, global_step=wandb_logging_step)\n","\n","        if args.push_to_hub:\n","            save_model_card(\n","                repo_id,\n","                images=images,\n","                base_model=args.pretrained_model_name_or_path,\n","                dataset_name=args.dataset_name,\n","                repo_folder=args.output_dir,\n","            )\n","            upload_folder(\n","                repo_id=repo_id,\n","                folder_path=args.output_dir,\n","                commit_message=\"End of training\",\n","                ignore_patterns=[\"step_*\", \"epoch_*\"],\n","            )\n","\n","    accelerator.end_training()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1e_GAiODXjDJ172nnEo_sIuxEzm-Afpjf","timestamp":1734503337817},{"file_id":"1wGNmFwrv7iM_UDv1sPkPgglRU6KPmfyJ","timestamp":1734320751571}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"708245e4b9954d0ca1131a05cd2da201":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d25e93227ed446ad85a8ff76ebd50cbf","IPY_MODEL_ccf77963afb54a449bc3c11235af81f5","IPY_MODEL_c77ec2d955924b1cb291612b5b79324a"],"layout":"IPY_MODEL_734545cbc5434f0aad07dc26d198d43e"}},"d25e93227ed446ad85a8ff76ebd50cbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e245716fb554e91809562a142b734c1","placeholder":"â€‹","style":"IPY_MODEL_df4fe1d436fa429f827d873c2aa4d063","value":""}},"ccf77963afb54a449bc3c11235af81f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a24e2fdcdd684bcd84d9b20017480029","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e18b9af3645a4a5184cc9e9e7553c690","value":0}},"c77ec2d955924b1cb291612b5b79324a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0846753e03204546a8ecc0b4c8065df9","placeholder":"â€‹","style":"IPY_MODEL_6bb0a080b7854944b9421b932c20ae40","value":"â€‡0/0â€‡[00:00&lt;?,â€‡?it/s]"}},"734545cbc5434f0aad07dc26d198d43e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e245716fb554e91809562a142b734c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df4fe1d436fa429f827d873c2aa4d063":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a24e2fdcdd684bcd84d9b20017480029":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e18b9af3645a4a5184cc9e9e7553c690":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0846753e03204546a8ecc0b4c8065df9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bb0a080b7854944b9421b932c20ae40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52fcc58c6e5340698ede98fedb2b99e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30f922f4f56d43daa5c1d8e6c3e30aa2","IPY_MODEL_209ed205a77945ac843a4eb7ab2f67f4","IPY_MODEL_d21f20a6fb654f9c8991441b869ae061"],"layout":"IPY_MODEL_a9d29a298bc94020b347b186b3f037ba"}},"30f922f4f56d43daa5c1d8e6c3e30aa2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42191a6eac514141a55d45cabb0bf070","placeholder":"â€‹","style":"IPY_MODEL_54bb120a2d944e72b1f82cc0a1f220fc","value":"Resolvingâ€‡dataâ€‡files:â€‡100%"}},"209ed205a77945ac843a4eb7ab2f67f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31240efdaefc47319c684dee98dcb1a6","max":60841,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01c2171e9aaf417f8832c98df6cafbbe","value":60841}},"d21f20a6fb654f9c8991441b869ae061":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9043727c210e404f88704771ff913ba8","placeholder":"â€‹","style":"IPY_MODEL_50e5b2abacb34c528d5de9dd67e9b631","value":"â€‡60841/60841â€‡[00:01&lt;00:00,â€‡189901.14it/s]"}},"a9d29a298bc94020b347b186b3f037ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42191a6eac514141a55d45cabb0bf070":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54bb120a2d944e72b1f82cc0a1f220fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31240efdaefc47319c684dee98dcb1a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01c2171e9aaf417f8832c98df6cafbbe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9043727c210e404f88704771ff913ba8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50e5b2abacb34c528d5de9dd67e9b631":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"662f5460f42a4f0baa44564dc23e15a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_214d74153de54064a266242571013289","IPY_MODEL_c0b314c451d240899d78b820fed77c80","IPY_MODEL_b4563ed1c9fb480480062fb3cd635b54"],"layout":"IPY_MODEL_c96d2e5eda12476e981986e59808817e"}},"214d74153de54064a266242571013289":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bd64f8a1eec477d9d40349771938641","placeholder":"â€‹","style":"IPY_MODEL_d73d581218ff4a7ca5084cde11769b88","value":"Loadingâ€‡pipelineâ€‡components...:â€‡100%"}},"c0b314c451d240899d78b820fed77c80":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_54ec6a7b15964f3da687040f956344b5","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ea00da510e4432ea69f14a318599f14","value":7}},"b4563ed1c9fb480480062fb3cd635b54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceec8859b20b4e6a832b1dbb2f7d5488","placeholder":"â€‹","style":"IPY_MODEL_1734bf7f48384ebba9399fa3daf91d29","value":"â€‡7/7â€‡[00:01&lt;00:00,â€‡â€‡5.83it/s]"}},"c96d2e5eda12476e981986e59808817e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bd64f8a1eec477d9d40349771938641":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d73d581218ff4a7ca5084cde11769b88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54ec6a7b15964f3da687040f956344b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ea00da510e4432ea69f14a318599f14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ceec8859b20b4e6a832b1dbb2f7d5488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1734bf7f48384ebba9399fa3daf91d29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9166eda25da48b38fc9150f608fcbd2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ca3cce261c94a02ac580aef759abbc8","IPY_MODEL_8f233051ddb742a190fe2f3d78a9701d","IPY_MODEL_67814d7a850b42c18dbc983065b8e35d"],"layout":"IPY_MODEL_7db766b6bbd34887a0de3cbdadc20ec1"}},"0ca3cce261c94a02ac580aef759abbc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b09ff9fd05274a8e9667bdb0e79c0205","placeholder":"â€‹","style":"IPY_MODEL_3b7cba5695ba4dbdb406518e76738555","value":"Loadingâ€‡pipelineâ€‡components...:â€‡100%"}},"8f233051ddb742a190fe2f3d78a9701d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b84d8b92ab65465ba85872adacfa5ad2","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c01b9c5b7c64599ad460b514d4d7e97","value":7}},"67814d7a850b42c18dbc983065b8e35d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2fb57ba379b4e45aafe44b462188eaa","placeholder":"â€‹","style":"IPY_MODEL_2544bbb330934cfebf7e18d67084f543","value":"â€‡7/7â€‡[00:01&lt;00:00,â€‡â€‡3.39it/s]"}},"7db766b6bbd34887a0de3cbdadc20ec1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b09ff9fd05274a8e9667bdb0e79c0205":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b7cba5695ba4dbdb406518e76738555":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b84d8b92ab65465ba85872adacfa5ad2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c01b9c5b7c64599ad460b514d4d7e97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2fb57ba379b4e45aafe44b462188eaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2544bbb330934cfebf7e18d67084f543":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be2285b8cc024fd7a04d6908aad9731f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_819a5ecf3c374ef79dd3929c78d31a91","IPY_MODEL_20e2140f05744197b054408221812f3e","IPY_MODEL_4034ae7513e04f79b77ec82fd7b6034f"],"layout":"IPY_MODEL_b7fd644f4b554d2a99b207d45cb6e519"}},"819a5ecf3c374ef79dd3929c78d31a91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94ff5d1109e44e96b2eb5216b8b722ef","placeholder":"â€‹","style":"IPY_MODEL_e5b1d6af502c40f0aa054f11d36a65fb","value":"Steps:â€‡â€‡95%"}},"20e2140f05744197b054408221812f3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b183c3ab2e864e69aebda4554b810649","max":3000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc3363a1851b43d8a0c878d6c0de2722","value":2849}},"4034ae7513e04f79b77ec82fd7b6034f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f55be18b63a840538f3d08dee0d66d96","placeholder":"â€‹","style":"IPY_MODEL_bc3a72e83bb94fe6bed3e8576a20263d","value":"â€‡2849/3000â€‡[48:34&lt;04:18,â€‡â€‡1.71s/it,â€‡lr=1.2e-5,â€‡step_loss=0.164]"}},"b7fd644f4b554d2a99b207d45cb6e519":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94ff5d1109e44e96b2eb5216b8b722ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5b1d6af502c40f0aa054f11d36a65fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b183c3ab2e864e69aebda4554b810649":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc3363a1851b43d8a0c878d6c0de2722":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f55be18b63a840538f3d08dee0d66d96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc3a72e83bb94fe6bed3e8576a20263d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16f91b37b889457d83a856c01258f1e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6cf9ed2458743cfba61d3ec6cbf6041","IPY_MODEL_4c233904d6814842ada362ad0b54f971","IPY_MODEL_a1850871c3b84f709bc9cc5d14b7aadd"],"layout":"IPY_MODEL_35faa8623e9643cc8933daeba5d36103"}},"a6cf9ed2458743cfba61d3ec6cbf6041":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_101a33f7020f4e29904a227370aecd23","placeholder":"â€‹","style":"IPY_MODEL_b8bee9e790064ca09f53cf2bcbe74e0d","value":"Loadingâ€‡pipelineâ€‡components...:â€‡100%"}},"4c233904d6814842ada362ad0b54f971":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7c38ee687ba4e1db14c911bbada3fb7","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3dc59323c43d474592df291ec6e8c720","value":7}},"a1850871c3b84f709bc9cc5d14b7aadd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbd9215c92c043fe8adf3dd565e466da","placeholder":"â€‹","style":"IPY_MODEL_37b39a0c03e34154947caf3a4c0baedb","value":"â€‡7/7â€‡[00:01&lt;00:00,â€‡â€‡4.99it/s]"}},"35faa8623e9643cc8933daeba5d36103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"101a33f7020f4e29904a227370aecd23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8bee9e790064ca09f53cf2bcbe74e0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7c38ee687ba4e1db14c911bbada3fb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dc59323c43d474592df291ec6e8c720":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dbd9215c92c043fe8adf3dd565e466da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37b39a0c03e34154947caf3a4c0baedb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09922d3957234d258d4bd708c8da4894":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_748136f70cde4f86992410ec6cf565a0","IPY_MODEL_7f12e253d261464e8b9f16b55608952e","IPY_MODEL_2539e00fe39f48789ed7c80ed66e34a8"],"layout":"IPY_MODEL_3a51a62565fd4c489cef9e77c5dd11e0"}},"748136f70cde4f86992410ec6cf565a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8912cd9f384f462cb6c3cdbc7b135ea9","placeholder":"â€‹","style":"IPY_MODEL_3ae3bb5b1aff40fbb406f5103cb2cb58","value":"Loadingâ€‡pipelineâ€‡components...:â€‡100%"}},"7f12e253d261464e8b9f16b55608952e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73c77a4500004ee69fd806568f595da1","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d3dee912e7e471d900f23a39f4c114e","value":7}},"2539e00fe39f48789ed7c80ed66e34a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32ee7b8752a845f58c93294574370791","placeholder":"â€‹","style":"IPY_MODEL_d02bdb18f31e469da22949bba69854cb","value":"â€‡7/7â€‡[00:01&lt;00:00,â€‡â€‡5.85it/s]"}},"3a51a62565fd4c489cef9e77c5dd11e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8912cd9f384f462cb6c3cdbc7b135ea9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ae3bb5b1aff40fbb406f5103cb2cb58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73c77a4500004ee69fd806568f595da1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d3dee912e7e471d900f23a39f4c114e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"32ee7b8752a845f58c93294574370791":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d02bdb18f31e469da22949bba69854cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4448b6c19d0942da9ea6d165faf7fa48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_957d694b9f1646a59a54cff777842dea","IPY_MODEL_dbc3c96dcf604ee1b8654ddad58909d4","IPY_MODEL_8035d6de11f646169728374b7ccba814"],"layout":"IPY_MODEL_c0e1ed8f9316457092d6539568d024b9"}},"957d694b9f1646a59a54cff777842dea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3d9bc0a03194ed3b371881598f37ad1","placeholder":"â€‹","style":"IPY_MODEL_f2fdc3475f40441f91a4f94728f53ab0","value":"Loadingâ€‡pipelineâ€‡components...:â€‡100%"}},"dbc3c96dcf604ee1b8654ddad58909d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c74fc042019147d8898149699838adf1","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7488ca13df04f568668ff04f35a55f2","value":7}},"8035d6de11f646169728374b7ccba814":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c87f32bb54d44676b69b2cd2cd744336","placeholder":"â€‹","style":"IPY_MODEL_feb84beefe13452580a2a871890643e7","value":"â€‡7/7â€‡[00:01&lt;00:00,â€‡â€‡5.27it/s]"}},"c0e1ed8f9316457092d6539568d024b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3d9bc0a03194ed3b371881598f37ad1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2fdc3475f40441f91a4f94728f53ab0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c74fc042019147d8898149699838adf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7488ca13df04f568668ff04f35a55f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c87f32bb54d44676b69b2cd2cd744336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"feb84beefe13452580a2a871890643e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79d49e6c9dc6452fa8d7e4b6d496207b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84c3c87faf544c44ac45d4fe13827067","IPY_MODEL_f03018064aab47ab8b81cd66eb2ca0e9","IPY_MODEL_294494be69b04753ab1e053bf0836509"],"layout":"IPY_MODEL_82d80e8766b44752addf05dfb1543eb2"}},"84c3c87faf544c44ac45d4fe13827067":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_332d0dccc6c343829aba89af985897d8","placeholder":"â€‹","style":"IPY_MODEL_e452e9cea9a740ac84f4d0a7c20dee39","value":"Loadingâ€‡pipelineâ€‡components...:â€‡100%"}},"f03018064aab47ab8b81cd66eb2ca0e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df54c1abb9ee4b1ab378b1a65451aee7","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52f71ef716854f4f9e23df1dbd24c6cb","value":7}},"294494be69b04753ab1e053bf0836509":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d15761701be24f28aadd90ca6519e337","placeholder":"â€‹","style":"IPY_MODEL_537bda4539164366a65d0d26ae5c683e","value":"â€‡7/7â€‡[00:01&lt;00:00,â€‡â€‡5.87it/s]"}},"82d80e8766b44752addf05dfb1543eb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"332d0dccc6c343829aba89af985897d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e452e9cea9a740ac84f4d0a7c20dee39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df54c1abb9ee4b1ab378b1a65451aee7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52f71ef716854f4f9e23df1dbd24c6cb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d15761701be24f28aadd90ca6519e337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"537bda4539164366a65d0d26ae5c683e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}