{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# New Section (Below is updated code from ChatGPT)"
      ],
      "metadata": {
        "id": "yNT5qie9Eq5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Fixes**\n",
        "* Data Augmentation: Increased the range of transformations (rotation, vertical\n",
        "flip, jitter).\n",
        "* Dropout: Added dropout layers to the model to help regularize it.\n",
        "* Early Stopping: Implemented early stopping to prevent overfitting by stopping training when the validation AUC doesn't improve for a certain number of epochs.\n",
        "* L2 Regularization: Added weight decay to the Adam optimizer to penalize large weights.\n",
        "* Lower Learning Rate: Reduced the learning rate to allow the model to train more smoothly and avoid overfitting."
      ],
      "metadata": {
        "id": "oAq1kPT9aT-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "huRhGz6Yzqi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915c011a-6a5d-45ca-f39f-c2c435f26e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist"
      ],
      "metadata": {
        "id": "bXF896TuzvD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5474ac6-a336-4c51-949c-08f22738680f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.5.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.24.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (11.0.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.20.1+cu121)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=861fd717752879eb67780707c9158049f568ef935a2967b0a9756e21c7fead78\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, medmnist\n",
            "Successfully installed fire-0.7.0 medmnist-3.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import resnet18\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_7CbacFtzxDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "01wjnBv8z04F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting occurs when the model performs well on the training data but poorly on the validation data. This usually happens when the model becomes too complex relative to the amount of data, or when the training data is not sufficiently diverse.\n",
        "\n",
        "To fix overfitting, here are several strategies you can try:\n",
        "Increase Data Augmentation: You can apply more diverse transformations to the images, which will help the model generalize better to unseen data.\n",
        "\n",
        "Use Dropout: Adding dropout layers to the model helps regularize it by randomly \"dropping\" neurons during training, forcing the model to not rely too heavily on any single feature.\n",
        "\n",
        "Reduce Model Complexity: ResNet18 is a relatively deep model. You could try using a smaller model like ResNet34 or even a simpler model architecture if overfitting is severe.\n",
        "\n",
        "Early Stopping: Monitor the validation loss or AUC and stop training if it stops improving. This will help avoid training for too long, which can cause overfitting.\n",
        "\n",
        "Regularization (L2 weight decay): Adding L2 regularization (weight decay) to the optimizer helps prevent the model from fitting too tightly to the training data.\n",
        "\n",
        "Reduce Learning Rate: If the learning rate is too high, the model might overfit due to too large updates during training.\n",
        "\n",
        "Here's how you can apply these techniques to your code:\n",
        "1. Increase Data Augmentation\n",
        "Add more augmentations to introduce more variability in the training set:"
      ],
      "metadata": {
        "id": "SLrgITKCZAMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomRotation(15),  # Increased rotation range\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),  # Add vertical flip\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jitter\n",
        "    transforms.RandomAffine(10, shear=5),  # Apply affine transformations\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize based on medical image domain\n",
        "])"
      ],
      "metadata": {
        "id": "XHtgwvePZH2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "class SingleLabelChestMNIST(Dataset):\n",
        "    def __init__(self, split, transform=None, size=224):\n",
        "        info = INFO['chestmnist']\n",
        "        DataClass = getattr(medmnist, info['python_class'])\n",
        "        self.data = DataClass(split=split, download=True, as_rgb=True, size=size)\n",
        "        self.imgs = self.data.imgs\n",
        "        self.labels = self.data.labels\n",
        "        self.transform = transform\n",
        "\n",
        "        # Filter out images that have more than one label\n",
        "        self.single_label_indices = [i for i in range(len(self.labels)) if self.labels[i].sum() == 1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.single_label_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.imgs[self.single_label_indices[idx]]\n",
        "        label = self.labels[self.single_label_indices[idx]].argmax()  # Convert one-hot to scalar label\n",
        "\n",
        "        # Convert NumPy array to PIL image\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "CUa3wp4Mz31K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data with the specified size\n",
        "batch_size = 64\n",
        "train_dataset = SingleLabelChestMNIST(split='train', transform=transform, size=224)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = SingleLabelChestMNIST(split='test', transform=transform, size=224)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "ZaznWuATz5sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25882640-3217-4714-e7ff-89f26b6e42d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://zenodo.org/records/10519652/files/chestmnist_224.npz?download=1 to /root/.medmnist/chestmnist_224.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.89G/3.89G [05:23<00:00, 12.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/chestmnist_224.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Add Dropout Layers to the Model\n",
        "You can modify the ResNet model by adding dropout layers after each block, or between the fully connected layers. Here's how to add dropout in the modified ChestMNISTModel class:"
      ],
      "metadata": {
        "id": "m3bhfczwZO1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChestMNISTModel(nn.Module):\n",
        "    def __init__(self, num_classes=14):\n",
        "        super(ChestMNISTModel, self).__init__()\n",
        "        self.model = resnet18(pretrained=True)\n",
        "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Modify for single channel\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)  # Modify output for 14 classes\n",
        "\n",
        "        # Add Dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.5)  # 50% dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.dropout(x)  # Apply dropout after the fully connected layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "PNNlGOHJZTM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* L2 Regularization (Weight Decay)\n",
        "You can add L2 regularization by passing a weight_decay parameter to the Adam optimizer\n",
        "\n",
        "* Reduce Learning Rate\n",
        "You can reduce the learning rate if the model seems to be overfitting. This can help the model converge more smoothly:"
      ],
      "metadata": {
        "id": "PQOXSInjZ6Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChestMNISTModel(num_classes=14).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 regularization (weight decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)"
      ],
      "metadata": {
        "id": "n5d5H7ruz8t4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c306d5-c9b8-426e-c5eb-8b905193bb0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 142MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Training function\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "lRYoDD6Xz_qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Validation function\n",
        "def validate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # For AUC calculation\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(test_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    # Calculate AUC\n",
        "    all_preds = torch.softmax(torch.tensor(all_preds), dim=1).numpy()\n",
        "    auc = roc_auc_score(all_labels, all_preds, multi_class='ovr')\n",
        "\n",
        "    return epoch_loss, epoch_acc, auc"
      ],
      "metadata": {
        "id": "eXPLFyCL0Ddt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use Early Stopping\n",
        "You can implement early stopping to stop training if the validation AUC doesn't improve for a specified number of epochs. Here's an example of how to add early stopping based on validation AUC:"
      ],
      "metadata": {
        "id": "JB_0oQk8ZhuZ"
      }
    },
    {
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Define the path to the saved model\n",
        "best_model_path = '/content/drive/MyDrive/best_model_chtgptR18.pth'\n",
        "\n",
        "# Step 1: Load the model if it exists, or initialize a new one if not\n",
        "def load_model(model, optimizer, scheduler, model_path):\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Loading saved model from {model_path}...\")\n",
        "        checkpoint = torch.load(model_path)  # Load the entire checkpoint dictionary\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])  # Load model weights using correct key\n",
        "\n",
        "        # If the checkpoint includes optimizer and scheduler states, load them as well\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "        # Optionally, load epoch counter if you saved it\n",
        "        epoch_start = checkpoint.get('epoch', 0)\n",
        "        best_auc = checkpoint.get('best_auc', 0.0)\n",
        "\n",
        "        print(\"Model loaded successfully. Continuing training...\")\n",
        "    else:\n",
        "        print(\"No saved model found, initializing a new model...\")\n",
        "        epoch_start = 0\n",
        "        best_auc = 0.0\n",
        "\n",
        "    return model, optimizer, scheduler, epoch_start, best_auc\n",
        "\n",
        "\n",
        "# Step 2: Save the model after each epoch\n",
        "def save_model(model, optimizer, scheduler, epoch, auc, model_path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_auc': auc\n",
        "    }, model_path)\n",
        "    print(f\"Model saved at epoch {epoch}, AUC: {auc:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "66wFr3hLhT11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model (if exists) and continue from there\n",
        "model, optimizer, scheduler, epoch_start, best_auc = load_model(model, optimizer, scheduler, best_model_path)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "patience = 10  # Early stopping patience\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(epoch_start, num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    # Validate the model\n",
        "    val_loss, val_acc, val_auc = validate(model, test_loader, criterion, device)\n",
        "\n",
        "    # Print metrics for the current epoch\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%, Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "    # Update the scheduler based on validation AUC\n",
        "    scheduler.step(val_auc)\n",
        "\n",
        "    # Save model if AUC improves\n",
        "    if val_auc > best_auc:\n",
        "        best_auc = val_auc\n",
        "        save_model(model, optimizer, scheduler, epoch + 1, best_auc, best_model_path)  # Save model at improved AUC\n",
        "        epochs_without_improvement = 0  # Reset counter\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    # Early stopping condition\n",
        "    if epochs_without_improvement >= patience:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GndStD6JemK7",
        "outputId": "01ebdb81-12ab-4407-858f-41e90a9fe3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved model found, initializing a new model...\n",
            "Epoch 1/1000\n",
            "Train Loss: 2.3659, Train Accuracy: 23.41%\n",
            "Validation Loss: 2.1719, Validation Accuracy: 30.82%, Validation AUC: 0.6530\n",
            "Model saved at epoch 1, AUC: 0.6530\n",
            "Epoch 2/1000\n",
            "Train Loss: 2.3664, Train Accuracy: 23.30%\n",
            "Validation Loss: 2.1745, Validation Accuracy: 33.06%, Validation AUC: 0.6720\n",
            "Model saved at epoch 2, AUC: 0.6720\n",
            "Epoch 3/1000\n",
            "Train Loss: 2.3519, Train Accuracy: 23.68%\n",
            "Validation Loss: 2.1866, Validation Accuracy: 33.10%, Validation AUC: 0.6550\n",
            "Epoch 4/1000\n",
            "Train Loss: 2.3295, Train Accuracy: 24.42%\n",
            "Validation Loss: 2.1699, Validation Accuracy: 33.92%, Validation AUC: 0.6840\n",
            "Model saved at epoch 4, AUC: 0.6840\n",
            "Epoch 5/1000\n",
            "Train Loss: 2.3166, Train Accuracy: 24.64%\n",
            "Validation Loss: 2.1371, Validation Accuracy: 32.64%, Validation AUC: 0.6920\n",
            "Model saved at epoch 5, AUC: 0.6920\n",
            "Epoch 6/1000\n",
            "Train Loss: 2.3116, Train Accuracy: 25.00%\n",
            "Validation Loss: 2.1273, Validation Accuracy: 33.55%, Validation AUC: 0.7019\n",
            "Model saved at epoch 6, AUC: 0.7019\n",
            "Epoch 7/1000\n",
            "Train Loss: 2.2894, Train Accuracy: 25.59%\n",
            "Validation Loss: 2.0777, Validation Accuracy: 36.03%, Validation AUC: 0.7000\n",
            "Epoch 8/1000\n",
            "Train Loss: 2.2817, Train Accuracy: 25.91%\n",
            "Validation Loss: 2.0472, Validation Accuracy: 36.44%, Validation AUC: 0.7124\n",
            "Model saved at epoch 8, AUC: 0.7124\n",
            "Epoch 9/1000\n",
            "Train Loss: 2.2795, Train Accuracy: 26.16%\n",
            "Validation Loss: 2.0641, Validation Accuracy: 35.52%, Validation AUC: 0.7027\n",
            "Epoch 10/1000\n",
            "Train Loss: 2.2638, Train Accuracy: 26.74%\n",
            "Validation Loss: 2.0300, Validation Accuracy: 36.51%, Validation AUC: 0.7223\n",
            "Model saved at epoch 10, AUC: 0.7223\n",
            "Epoch 11/1000\n",
            "Train Loss: 2.2468, Train Accuracy: 27.29%\n",
            "Validation Loss: 2.0738, Validation Accuracy: 36.22%, Validation AUC: 0.7190\n",
            "Epoch 12/1000\n",
            "Train Loss: 2.2491, Train Accuracy: 26.84%\n",
            "Validation Loss: 2.0238, Validation Accuracy: 35.98%, Validation AUC: 0.7299\n",
            "Model saved at epoch 12, AUC: 0.7299\n",
            "Epoch 13/1000\n",
            "Train Loss: 2.2452, Train Accuracy: 27.49%\n",
            "Validation Loss: 2.0646, Validation Accuracy: 36.68%, Validation AUC: 0.7322\n",
            "Model saved at epoch 13, AUC: 0.7322\n",
            "Epoch 14/1000\n",
            "Train Loss: 2.2404, Train Accuracy: 27.19%\n",
            "Validation Loss: 2.0207, Validation Accuracy: 38.73%, Validation AUC: 0.7398\n",
            "Model saved at epoch 14, AUC: 0.7398\n",
            "Epoch 15/1000\n",
            "Train Loss: 2.2365, Train Accuracy: 27.12%\n",
            "Validation Loss: 1.9972, Validation Accuracy: 38.30%, Validation AUC: 0.7385\n",
            "Epoch 16/1000\n",
            "Train Loss: 2.2313, Train Accuracy: 27.66%\n",
            "Validation Loss: 1.9998, Validation Accuracy: 37.96%, Validation AUC: 0.7391\n",
            "Epoch 17/1000\n",
            "Train Loss: 2.2231, Train Accuracy: 28.05%\n",
            "Validation Loss: 2.0749, Validation Accuracy: 36.20%, Validation AUC: 0.7186\n",
            "Epoch 18/1000\n",
            "Train Loss: 2.2241, Train Accuracy: 27.67%\n",
            "Validation Loss: 2.0406, Validation Accuracy: 37.16%, Validation AUC: 0.7306\n",
            "Epoch 19/1000\n",
            "Train Loss: 2.2148, Train Accuracy: 28.15%\n",
            "Validation Loss: 1.9982, Validation Accuracy: 38.89%, Validation AUC: 0.7384\n",
            "Epoch 20/1000\n",
            "Train Loss: 2.2058, Train Accuracy: 28.48%\n",
            "Validation Loss: 1.9675, Validation Accuracy: 38.94%, Validation AUC: 0.7457\n",
            "Model saved at epoch 20, AUC: 0.7457\n",
            "Epoch 21/1000\n",
            "Train Loss: 2.2074, Train Accuracy: 28.03%\n",
            "Validation Loss: 1.9740, Validation Accuracy: 39.59%, Validation AUC: 0.7575\n",
            "Model saved at epoch 21, AUC: 0.7575\n",
            "Epoch 22/1000\n",
            "Train Loss: 2.1983, Train Accuracy: 28.86%\n",
            "Validation Loss: 1.9849, Validation Accuracy: 38.47%, Validation AUC: 0.7430\n",
            "Epoch 23/1000\n",
            "Train Loss: 2.1873, Train Accuracy: 29.25%\n",
            "Validation Loss: 1.9635, Validation Accuracy: 39.83%, Validation AUC: 0.7568\n",
            "Epoch 24/1000\n",
            "Train Loss: 2.1841, Train Accuracy: 29.14%\n",
            "Validation Loss: 1.9512, Validation Accuracy: 40.02%, Validation AUC: 0.7576\n",
            "Model saved at epoch 24, AUC: 0.7576\n",
            "Epoch 25/1000\n",
            "Train Loss: 2.1829, Train Accuracy: 28.77%\n",
            "Validation Loss: 1.9342, Validation Accuracy: 40.53%, Validation AUC: 0.7688\n",
            "Model saved at epoch 25, AUC: 0.7688\n",
            "Epoch 26/1000\n",
            "Train Loss: 2.1646, Train Accuracy: 29.78%\n",
            "Validation Loss: 1.9407, Validation Accuracy: 39.10%, Validation AUC: 0.7522\n",
            "Epoch 27/1000\n",
            "Train Loss: 2.1684, Train Accuracy: 29.60%\n",
            "Validation Loss: 1.9349, Validation Accuracy: 40.25%, Validation AUC: 0.7561\n",
            "Epoch 28/1000\n",
            "Train Loss: 2.1678, Train Accuracy: 29.97%\n",
            "Validation Loss: 1.9680, Validation Accuracy: 40.50%, Validation AUC: 0.7688\n",
            "Epoch 29/1000\n",
            "Train Loss: 2.1617, Train Accuracy: 29.83%\n",
            "Validation Loss: 1.9428, Validation Accuracy: 39.51%, Validation AUC: 0.7557\n",
            "Epoch 30/1000\n",
            "Train Loss: 2.1586, Train Accuracy: 29.85%\n",
            "Validation Loss: 1.9513, Validation Accuracy: 39.69%, Validation AUC: 0.7592\n",
            "Epoch 31/1000\n",
            "Train Loss: 2.1501, Train Accuracy: 30.12%\n",
            "Validation Loss: 1.9432, Validation Accuracy: 40.82%, Validation AUC: 0.7590\n",
            "Epoch 32/1000\n",
            "Train Loss: 2.1184, Train Accuracy: 31.17%\n",
            "Validation Loss: 1.9011, Validation Accuracy: 41.68%, Validation AUC: 0.7775\n",
            "Model saved at epoch 32, AUC: 0.7775\n",
            "Epoch 33/1000\n",
            "Train Loss: 2.1136, Train Accuracy: 31.32%\n",
            "Validation Loss: 1.8895, Validation Accuracy: 41.83%, Validation AUC: 0.7724\n",
            "Epoch 34/1000\n",
            "Train Loss: 2.1085, Train Accuracy: 31.12%\n",
            "Validation Loss: 1.8996, Validation Accuracy: 41.30%, Validation AUC: 0.7731\n",
            "Epoch 35/1000\n",
            "Train Loss: 2.1083, Train Accuracy: 31.54%\n",
            "Validation Loss: 1.8950, Validation Accuracy: 41.27%, Validation AUC: 0.7730\n",
            "Epoch 36/1000\n",
            "Train Loss: 2.1017, Train Accuracy: 31.73%\n",
            "Validation Loss: 1.9035, Validation Accuracy: 41.88%, Validation AUC: 0.7732\n",
            "Epoch 37/1000\n",
            "Train Loss: 2.0915, Train Accuracy: 32.12%\n",
            "Validation Loss: 1.8995, Validation Accuracy: 41.49%, Validation AUC: 0.7721\n",
            "Epoch 38/1000\n",
            "Train Loss: 2.0936, Train Accuracy: 31.83%\n",
            "Validation Loss: 1.8824, Validation Accuracy: 42.00%, Validation AUC: 0.7776\n",
            "Model saved at epoch 38, AUC: 0.7776\n",
            "Epoch 39/1000\n",
            "Train Loss: 2.0611, Train Accuracy: 33.03%\n",
            "Validation Loss: 1.8426, Validation Accuracy: 42.51%, Validation AUC: 0.7814\n",
            "Model saved at epoch 39, AUC: 0.7814\n",
            "Epoch 40/1000\n",
            "Train Loss: 2.0663, Train Accuracy: 32.63%\n",
            "Validation Loss: 1.8843, Validation Accuracy: 41.78%, Validation AUC: 0.7776\n",
            "Epoch 41/1000\n",
            "Train Loss: 2.0601, Train Accuracy: 32.70%\n",
            "Validation Loss: 1.8683, Validation Accuracy: 41.72%, Validation AUC: 0.7735\n",
            "Epoch 42/1000\n",
            "Train Loss: 2.0541, Train Accuracy: 33.54%\n",
            "Validation Loss: 1.8524, Validation Accuracy: 41.75%, Validation AUC: 0.7792\n",
            "Epoch 43/1000\n",
            "Train Loss: 2.0434, Train Accuracy: 33.28%\n",
            "Validation Loss: 1.8542, Validation Accuracy: 41.99%, Validation AUC: 0.7800\n",
            "Epoch 44/1000\n",
            "Train Loss: 2.0316, Train Accuracy: 33.82%\n",
            "Validation Loss: 1.8600, Validation Accuracy: 42.56%, Validation AUC: 0.7776\n",
            "Epoch 45/1000\n",
            "Train Loss: 2.0412, Train Accuracy: 33.45%\n",
            "Validation Loss: 1.8514, Validation Accuracy: 42.12%, Validation AUC: 0.7789\n",
            "Epoch 46/1000\n",
            "Train Loss: 2.0412, Train Accuracy: 33.45%\n",
            "Validation Loss: 1.8514, Validation Accuracy: 42.12%, Validation AUC: 0.7789\n",
            "Epoch 46/1000\n",
            "Train Loss: 2.0180, Train Accuracy: 34.38%\n",
            "Validation Loss: 1.8417, Validation Accuracy: 42.69%, Validation AUC: 0.7794\n",
            "Epoch 47/1000\n",
            "Train Loss: 2.0180, Train Accuracy: 34.38%\n",
            "Validation Loss: 1.8417, Validation Accuracy: 42.69%, Validation AUC: 0.7794\n",
            "Epoch 47/1000\n",
            "Train Loss: 2.0191, Train Accuracy: 34.19%\n",
            "Validation Loss: 1.8521, Validation Accuracy: 42.79%, Validation AUC: 0.7806\n",
            "Epoch 48/1000\n",
            "Train Loss: 2.0191, Train Accuracy: 34.19%\n",
            "Validation Loss: 1.8521, Validation Accuracy: 42.79%, Validation AUC: 0.7806\n",
            "Epoch 48/1000\n",
            "Train Loss: 2.0127, Train Accuracy: 34.39%\n",
            "Validation Loss: 1.8457, Validation Accuracy: 41.75%, Validation AUC: 0.7740\n",
            "Epoch 49/1000\n",
            "Train Loss: 2.0127, Train Accuracy: 34.39%\n",
            "Validation Loss: 1.8457, Validation Accuracy: 41.75%, Validation AUC: 0.7740\n",
            "Epoch 49/1000\n",
            "Train Loss: 2.0132, Train Accuracy: 34.01%\n",
            "Validation Loss: 1.8600, Validation Accuracy: 41.78%, Validation AUC: 0.7778\n",
            "Early stopping triggered!\n",
            "Train Loss: 2.0132, Train Accuracy: 34.01%\n",
            "Validation Loss: 1.8600, Validation Accuracy: 41.78%, Validation AUC: 0.7778\n",
            "Early stopping triggered!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}